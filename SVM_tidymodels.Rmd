---
title: "SVM_tidymodels"
author: "liuc"
date: '2022-05-26'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## SVM_tidymodels

> https://scikit-learn.org/stable/modules/svm.html#svm-regression
> https://github.com/thebioengineer/TidyX


支持向量机(Support vector machines)可以处理complex classification, regression, and outlier detection problems。 支持向量指的是在svm寻找超平面时，只需要让最靠近中间分割线的那些点尽量远离，即只用到那些「支持向量 support vector」的样本。核函数是很重要的。把一个数据集正确分开的超平面可能有多个，而具有「最大间隔」的超平面就是 SVM 所要找的最优解。最靠近超平面的样本点即为「支持向量」。支持向量到超平面的距离称为「间隔 margin」。
简单点讲，SVM就是一种二类分类模型，他的基本模型是定义在特征空间上的间隔最大的线性分类器，SVM的学习策略就是间隔最大化。

1.线性可分SVM,
当训练数据线性可分时，通过硬间隔hard margin最大化可以学习得到一个线性分类器，即硬间隔SVM
2.线性SVM,
当训练数据不能线性可分但是可以近似线性可分时，通过软间隔(soft margin)最大化也可以学习到一个线性分类器，即软间隔SVM。
3.非线性SVM,
当训练数据线性不可分时，通过使用核技巧(kernel trick)和软间隔最大化，可以学习到一个非线性SVM。


SVM善于处理高纬度数据，比如基因表达数据集、微生物组OTU数据集等。

_The disadvantages of support vector machines include:_
If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.
SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).

*支持向量机的优点是:*
由于SVM是一个凸优化问题，所以求得的解一定是全局最优而不是局部最优。
不仅适用于线性线性问题还适用于非线性问题(用核技巧)。
拥有高维样本空间的数据也能用SVM，这是因为数据集的复杂度只取决于支持向量而不是数据集的维度，这在某种意义上避免了“维数灾难”。
理论基础比较完善(例如神经网络就更像一个黑盒子)。
*支持向量机的缺点是:*
二次规划问题求解将涉及m阶矩阵的计算(m为样本的个数), 因此SVM不适用于超大数据集。(SMO算法可以缓解这个问题)
只适用于二分类问题。(SVM的推广SVR也适用于回归问题；可以通过多个SVM的组合来解决多分类问题)


*SVM重要的超参数：*cost(错误项的惩罚参数), margin()



*做为一个调包选手，机器学习意味着什么呢*意味着何种算法适合何种数据，输入数据有什么要求，需要进行哪些特征工程，每个模型的解释有啥其自身的特点，具体的模型有哪些需要关注的超参数，具体的超参数如何设置范围、需不需要依据具体的数据集进行超参数的设置等等。剩下的就是真正在用的过程中不断的优化、优化、优化。


`sklearn`Python包中支持的SVM种类很多，`svm.LinearSVC`,`svm.SVC`,`svm.NuSVC`,`svm.OneClassSVM`等等，可以看到 SVM 在 sklearn 上有三个接口，分别是 LinearSVC、SVC 和 NuSVC。最常用的一般是 SVC 接口。除了特别表明是线性的两个类 LinearSVC 和 LinearSVR 之外，其他的所有类都是同时支持线性和非线性的。NuSVC和NuSVC 可以手动调节支持向量的数目， 其他参数都与最常用的SVC和SVR一致。而OneClassSVM 则是无监督的类。在R中其对应的函数为，



```{r, include=FALSE}
library(tidyverse)
library(tidymodels)
library(usemodels)

```


在建立模型之前，可以通过一些`tidymodels`所提供的辅助工具进行一些初步的工作，比如我们打算在前列腺癌表达数据集上进行重要变量（基因）的筛选和建立区分癌和癌旁的模型，并打算利用SVM算法，那么首先可以利用`usemodels`建立一个基本的模型框架，并基于具体的数据进行一些修改, 不过`usemodels`所支持的模型还有限，`parsnip::parsnip_addin()`也有一段时间没有更新了。
`show_engines()`函数可以查看所支持的引擎。
```{r}
usemodels::use_kernlab_svm_rbf(formula = class ~ ., data = prostat_train,
                               prefix = 'kernlab', verbose = F,
                               tune = TRUE, colors = TRUE, clipboard = F
                               )
```

### 一个简单的示例


准备数据
```{r, include=FALSE}
expr_file <- "datasets/prostat.expr.symbol.txt"
metadata_file <- "datasets/prostat.metadata.txt"

expr_mat <- read_delim(expr_file, delim = '\t') %>% 
  janitor::clean_names()
metadata <- read_delim(metadata_file, delim = '\t') %>% 
  janitor::clean_names()

# 此处或可以加上`Boruta`的结果进行一些变量筛选工作
input_data <- expr_mat %>% column_to_rownames('symbol') %>% t() %>% 
  as.data.frame() %>% rownames_to_column() %>% as_tibble() %>% 
  janitor::clean_names() %>% 
  left_join(metadata, by = c('rowname'='sample')) %>% 
  mutate(class = if_else(class=='tumor', 1, 0)) %>% 
  mutate(class = as_factor(class))


# 此处也可以保留rowname列，在下文中通过recipe中的`update_role`进行
set.seed(42)
df_split <- initial_split(input_data %>% select(-rowname))

prostat_train <- training(df_split)
prostat_test <- testing(df_split)
```



svm_linear主要由两个R包提供内在的引擎`show_engines('svm_linear')`: kernlab & LiblineaR

`svm_linear`, `svm_poly`, `svm_rbf` 其分别和`scikit-learn`中对应的关系为,`linear`：线性核函数, `poly`：多项式核函数,
`rbf`：径像核函数/高斯核, `sigmod`：sigmod 核函数.

`?kernlab::ksvm`中有提供的很多的kernal选择，但是`parsnip`提供的就比较有限了。
```{r}
prostat_folds <- vfold_cv(prostat_train, strata = class)


svm_spec <- svm_linear(mode = "classification") %>% 
  set_engine('kernlab')

# svm_spec <- svm_rbf(mode = 'classification', 
#                     cost = tune(), rbf_sigma = tune()) %>% 
#   set_engine('kernlab')

prostat_rec <-
  recipe(class ~ ., data = prostat_train)

## just to see how it is working:
# prep(prostat_rec) %>% bake(new_data = NULL) %>% glimpse()

prostat_wf <- workflow(prostat_rec, svm_spec)
prostat_wf
```



```{r}
doParallel::registerDoParallel()

set.seed(123)
prostat_metrics <- metric_set(accuracy, sens, spec)

# `fit_resamples`不可以做tune的操作，即不可以做网格搜索；但提供一种类似交叉验证的操作
prostat_rs <- fit_resamples(prostat_wf, resamples = prostat_folds, 
                            metrics = prostat_metrics)

collect_metrics(prostat_rs)
```


last_fit() to fit one final time to the training data and evaluate one final time on the testing data.
以下结果显示在测试集数据上具有不错的表现。
```{r}
final_rs <- last_fit(prostat_wf, df_split, metrics = prostat_metrics)


collect_metrics(final_rs)
```


confusion matrix:
在测试集数据上的混淆矩阵
```{r}
(collect_predictions(final_rs) %>%
  conf_mat(class, .pred_class)) %>%
  autoplot()
```


If we decide this model is good to go and we want to use it in the future, we can extract out the fitted workflow. This object can be used for prediction:
```{r}
final_fitted <- extract_workflow(final_rs)

# 测试集数据上的验证
augment(final_fitted, new_data = slice_sample(prostat_test, n = 3)) %>% 
  dplyr::select(last_col(5:1))
```


We can also examine this model (which is just linear with coefficients) to understand what drives its predictions.
在engine选择为LiblineaR时tidy函数支持，不过kernlab不支持。

```{r}
tidy(final_fitted) %>%
  slice_max(abs(estimate), n = 20) %>%
  mutate(
    term = str_remove_all(term, "tf_author_"),
    term = fct_reorder(term, abs(estimate))
  ) %>%
  ggplot(aes(x = abs(estimate), y = term, fill = estimate > 0)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  scale_fill_discrete(labels = c("Fewer weeks", "More weeks")) +
  labs(x = "Estimate from linear SVM (absolute value)", y = NULL, 
       fill = "How many weeks on\nbestseller list?")
```


### 带网格搜索，交叉验证的示例

所使用数据依旧为上述数据。

```{r}
prostat_folds <- vfold_cv(prostat_train, strata = class)


svm_spec <- svm_rbf(mode = 'classification', 
                    cost = tune(), 
                    rbf_sigma = tune()) %>% 
  set_engine('kernlab')

prostat_rec <-
  recipe(class ~ ., data = prostat_train) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) 


prostat_wf <- workflow(prostat_rec, svm_spec)
prostat_wf
```


```{r}
svm_grid <- grid_max_entropy()
```








