---
title: "PLS-DA"
format: html
---

## PLS DA

> https://www.statology.org/partial-least-squares-in-r/

*DA(Discriminant Analysis)*, 判别分析，是一种统计学习方法，旨在寻找将观测数据分为不同类别的最佳决策边界。它是一种有监督学习方法，需要已知观测数据所属的类别信息。 DA 的目标是找到一组线性组合，能够将不同类别的数据分离得最好。DA 的优点在于它可以找到最佳的决策边界，从而对新的观测数据进行分类。此外，DA可以处理多个自变量同时影响输出变量的情况。但是，DA 对输入变量之间的相关性比较敏感，如果输入变量高度相关，可能会出现过拟合现象。


*PLS-DA (Partial Least Squares Discriminant Analysis)* 是一种用于分类问题的统计学习方法。它可以同时考虑多个自变量之间的相关性，克服了传统的线性判别分析（LDA）在自变量高度相关的情况下容易出现的过拟合现象。PLS-DA 是 PLS （Partial Least Squares） 的一种变体，PLS 是一种主成分回归方法，旨在通过线性组合来提取输入变量中的信息，并建立它们与输出变量之间的关系。与主成分回归方法类似，PLS-DA 将输入变量投影到一个低维空间中，但它是针对分类问题的。在 PLS-DA 中，我们首先根据输入变量的协方差矩阵来计算一组新的变量，称为潜在变量。潜在变量是输入变量的线性组合，并且它们在输入变量中的方差最大，同时与输出变量之间的相关性也最大。我们可以通过不断地计算潜在变量来逐步提高分类模型的准确度。
PLS-DA 的优点在于它能够同时考虑输入变量之间的相关性以及它们与输出变量之间的关系。这使得它能够处理高度相关的输入变量，同时避免出现传统 LDA 中的过拟合现象。此外，PLS-DA可以很好地处理高维数据，因为它可以将数据投影到一个低维空间中，并保留尽可能多的信息。


*PLS (Partial Least Squares)* 是一种回归分析方法，旨在通过线性组合来提取输入变量中的信息，并建立它们与输出变量之间的关系。与传统的多元线性回归（MLR）方法不同，PLS不是直接将输入变量与输出变量拟合到一个线性方程中，而是通过一系列潜在变量来描述输入和输出变量之间的关系。在 PLS 中，我们首先根据输入变量的协方差矩阵来计算一组新的变量，称为潜在变量。潜在变量是输入变量的线性组合，并且它们在输入变量中的方差最大，同时与输出变量之间的相关性也最大。我们可以通过不断地计算潜在变量来逐步提高回归模型的准确度。PLS可以用来处理变量间相关性，共线性问题的数据，比如其经常用在代谢组、微生物组中等。

- Standardize both the predictor and response variables.
- Calculate M linear combinations (called “PLS components”) of the original p predictor variables that explain a significant amount of variation in both the response variable and the predictor variables.
- Use the method of least squares to fit a linear regression model using the PLS components as predictors.
- Use k-fold cross-validation to find the optimal number of PLS components to keep in the model.


*The principal component regression (PCR)* first applies Principal Component Analysis on the data set to summarize the original predictor variables into few new variables also known as principal components (PCs), which are a linear combination of the original data.
These PCs are then used to build the linear regression model. The number of principal components, to incorporate in the model, is chosen by cross-validation (cv). Note that, PCR is suitable when the data set contains highly correlated predictors. 不过PCR方法所得到的PC不能好好的解释outcome。


## 代码示例

```{r, include=FALSE}
library(tidyverse)
library(MASS)
library(pls)
library(vegan)
library(FactoMineR)
```


### LDA


准备测试数据，在具体的数据中
```{r}
data(iris)

# 设置响应变量为 Species，将其转换为因子变量
iris$Species <- as.factor(iris$Species)

# 拆分数据集为训练集和测试集
set.seed(123)
trainIndex <- caret::createDataPartition(iris$Species, p = 0.7, list = FALSE)
trainData <- iris[trainIndex, ]
testData <- iris[-trainIndex, ]

```


```{r}
# 进行 LDA 分析
lda.fit <- MASS::lda(Species ~ ., data = trainData)

# 预测测试集
lda.pred <- predict(lda.fit, testData[, 1:4])

# 计算预测准确度
accuracy <- mean(lda.pred$class == testData$Species)
cat("LDA accuracy is", round(accuracy, 2))

```

```{r}
# 将预测结果和实际结果合并
results <- data.frame(testData[, "Species"], lda.pred$class)

p <- predict(lda.fit, trainData[, 1:4])

tab <- table(Predicted = p$class, Actual = trainData$Species)
tab

```
```{r}
sum(diag(tab))/sum(tab)
```


```{r}

ldahist(data = p$x[,1], g = trainData$Species)
```
*interpret the result: *It’s clearly evident that no overlaps between first and second and first and third species. But some overlap observed between the second and third species.

```{r}
ldahist(data = p$x[,2], g = trainData$Species)
```
*interpret the result: *histogram based on lda2 showing complete overlap and its not good.

```{r}
klaR::partimat(Species~., data = trainData, method = "lda")
```



### PLS

There are two main algorithms for PLS, NIPALS and SIMPLS.

分类和回归问题都可以处理。

```{r}
# ?pls::gasoline
data("Boston", package = "MASS")

# 将数据集分为训练集和测试集
training.samples <- Boston$medv %>%
  caret::createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]
```

```{r}
# 进行 PLS 分析， 一个回归问题
# ?pls::plsr
# ncomp 参数的意思为，如何得到准确的ncomp值？
pls.fit <- pls::plsr(medv ~ ., ncomp = 5, data = train.data,
                     scale=TRUE, #This ensures that no predictor variable is overly influential in the model if it happens to be measured in different units.
                     validation="CV" #use k-fold cross-validation to evaluate the performance of the model
                     )

pls_null <- pls::plsr(medv ~ ., data = train.data, scale=TRUE, validation="CV")


```


looking at the test root mean squared error (test RMSE) calculated by the k-fold cross-validation to determine the number of PLS components worth keeping.

```{r}
summary(pls_null)
```
*从输出结果中找到* VALIDATION: RMSEP(test RMSE calculated by the k-fold cross validation) 中 随着comps数变化的CV值，CV值越小越好；TRAINING: % variance explained(percentage of the variance in the response variable explained by the PLS components).

```{r}
#visualize cross-validation plots
validationplot(pls_null)
validationplot(pls_null, val.type="MSEP")
validationplot(pls_null, val.type="R2")
```



```{r}
# 对于回归问题可以计算其混淆矩阵  
pls.pred <- predict(pls.fit, test.data)

# 计算预测准确率
table(round(pls.pred), test.data$Q)
```


```{r}
# 绘制 PLS 结果图
pls.pred <- predict(pls.fit, test.data)


plot(pls.fit, type = "line", col = "red", lwd = 2)
plot(pls.pred, col = "blue", pch = 19, cex = 0.7, main = "PLS Results")

```



*利用vegan包展开PLS分析：*

redundancy analysis,
```{r}
# 进行 PLS 分析
pls.fit <- vegan::rda(trainData[,-1], scale = TRUE, method = "pls", k = 5, Y = trainData[,1])
pls.pred <- predict(pls.fit, testData[, -1])

# 计算预测准确率
table(round(pls.pred$Y), testData$Q)

# 绘制 PLS 结果图
plot(pls.fit, type = "biplot", scaling = 3)
ordiplot(pls.fit, display = "sites", type = "n")
points(pls.fit, display = "sites", pch = 19, col = as.numeric(testData$Q) + 1, cex = 0.7)

# 显示 PLS 分析的摘要信息
summary(pls.fit)

```


### PLS-DA

```{r}
# 设置响应变量为 Species，并选择 3 个潜在变量
plsda.fit <- pls::plsr(Species ~ ., data = trainData, ncomp = 3)

# 对测试集进行预测
plsda.pred <- predict(plsda.fit, newdata = testData)

# 计算预测准确度
accuracy <- mean(plsda.pred$class == testData$Species)
cat("PLS-DA accuracy is", round(accuracy, 2))
```


*vegan包的分析*

```{r}
# 进行 PLS-DA 分析
plsda.fit <- vegan::rda(scale(iris[, 1:4]), iris$Species, method = "pls", nf = 3)

# 预测测试集
plsda.pred <- predict(plsda.fit, testData[, 1:4], display = "sites")

# 计算预测准确度
accuracy <- mean(plsda.pred$classes == testData$Species)
cat("PLS-DA accuracy is", round(accuracy, 2))

```













