---
title: "PLS-DA"
format: html
---

## PLS DA

DA(Discriminant Analysis), 判别分析，是一种统计学习方法，旨在寻找将观测数据分为不同类别的最佳决策边界。它是一种有监督学习方法，需要已知观测数据所属的类别信息。 DA 的目标是找到一组线性组合，能够将不同类别的数据分离得最好。DA 的优点在于它可以找到最佳的决策边界，从而对新的观测数据进行分类。此外，DA可以处理多个自变量同时影响输出变量的情况。但是，DA 对输入变量之间的相关性比较敏感，如果输入变量高度相关，可能会出现过拟合现象。


PLS-DA (Partial Least Squares Discriminant Analysis) 是一种用于分类问题的统计学习方法。它可以同时考虑多个自变量之间的相关性，克服了传统的线性判别分析（LDA）在自变量高度相关的情况下容易出现的过拟合现象。PLS-DA 是 PLS （Partial Least Squares） 的一种变体，PLS 是一种主成分回归方法，旨在通过线性组合来提取输入变量中的信息，并建立它们与输出变量之间的关系。与主成分回归方法类似，PLS-DA 将输入变量投影到一个低维空间中，但它是针对分类问题的。在 PLS-DA 中，我们首先根据输入变量的协方差矩阵来计算一组新的变量，称为潜在变量。潜在变量是输入变量的线性组合，并且它们在输入变量中的方差最大，同时与输出变量之间的相关性也最大。我们可以通过不断地计算潜在变量来逐步提高分类模型的准确度。
PLS-DA 的优点在于它能够同时考虑输入变量之间的相关性以及它们与输出变量之间的关系。这使得它能够处理高度相关的输入变量，同时避免出现传统 LDA 中的过拟合现象。此外，PLS-DA可以很好地处理高维数据，因为它可以将数据投影到一个低维空间中，并保留尽可能多的信息。


PLS (Partial Least Squares) 是一种回归分析方法，旨在通过线性组合来提取输入变量中的信息，并建立它们与输出变量之间的关系。与传统的多元线性回归（MLR）方法不同，PLS不是直接将输入变量与输出变量拟合到一个线性方程中，而是通过一系列潜在变量来描述输入和输出变量之间的关系。在 PLS 中，我们首先根据输入变量的协方差矩阵来计算一组新的变量，称为潜在变量。潜在变量是输入变量的线性组合，并且它们在输入变量中的方差最大，同时与输出变量之间的相关性也最大。我们可以通过不断地计算潜在变量来逐步提高回归模型的准确度。


## 代码示例

```{r, include=FALSE}
library(tidyverse)
library(MASS)
library(pls)
library(vegan)
library(FactoMineR)
```


### LDA


准备测试数据，在具体的数据中
```{r}
data(iris)

# 设置响应变量为 Species，将其转换为因子变量
iris$Species <- as.factor(iris$Species)

# 拆分数据集为训练集和测试集
set.seed(123)
trainIndex <- caret::createDataPartition(iris$Species, p = 0.7, list = FALSE)
trainData <- iris[trainIndex, ]
testData <- iris[-trainIndex, ]

```


```{r}
# 进行 LDA 分析
lda.fit <- MASS::lda(Species ~ ., data = trainData)

# 预测测试集
lda.pred <- predict(lda.fit, testData[, 1:4])

# 计算预测准确度
accuracy <- mean(lda.pred$class == testData$Species)
cat("LDA accuracy is", round(accuracy, 2))

```

```{r}
# 将预测结果和实际结果合并
results <- data.frame(testData[, "Species"], lda.pred$class)

# 绘制分类结果的散点图
ggplot(results, aes(x = Sepal.Length, y = Sepal.Width, color = lda.pred$class)) + 
  geom_point(size = 3) + 
  ggtitle("LDA classification of iris data") + 
  theme_bw()

```


*用vegan包进行分析*
```{r}
# 进行 LDA 分析
lda.fit <- vegan::lda(as.matrix(trainData[, 1:4]), trainData$Species)

# 预测测试集
lda.pred <- predict(lda.fit, as.matrix(testData[, 1:4]))

# 计算预测准确度
accuracy <- mean(lda.pred$class == testData$Species)
cat("LDA accuracy is", round(accuracy, 2))

```


### PLS

```{r}
# ?pls::gasoline
data(gasoline)

data(wine)

# 将数据集分为训练集和测试集
train <- sample(nrow(wine), nrow(wine) * 0.7)
trainData <- wine[train, ]
testData <- wine[-train, ]
```

```{r}
# 进行 PLS 分析
pls.fit <- pls::plsr(Q ~ ., ncomp = 5, data = trainData)
pls.pred <- predict(pls.fit, testData)

# 计算预测准确率
table(round(pls.pred), testData$Q)

# 绘制 PLS 结果图
plot(pls.fit, type = "line", col = "red", lwd = 2)
plot(pls.pred, col = "blue", pch = 19, cex = 0.7, main = "PLS Results")

# 显示 PLS 分析的摘要信息
summary(pls.fit)
```


*利用vegan包展开PLS分析：*
```{r}
# 进行 PLS 分析
pls.fit <- rda(trainData[,-1], scale = TRUE, method = "pls", k = 5, Y = trainData[,1])
pls.pred <- predict(pls.fit, testData[, -1])

# 计算预测准确率
table(round(pls.pred$Y), testData$Q)

# 绘制 PLS 结果图
plot(pls.fit, type = "biplot", scaling = 3)
ordiplot(pls.fit, display = "sites", type = "n")
points(pls.fit, display = "sites", pch = 19, col = as.numeric(testData$Q) + 1, cex = 0.7)

# 显示 PLS 分析的摘要信息
summary(pls.fit)

```


### PLS-DA

```{r}
# 设置响应变量为 Species，并选择 3 个潜在变量
plsda.fit <- pls::plsr(Species ~ ., data = trainData, ncomp = 3)

# 对测试集进行预测
plsda.pred <- predict(plsda.fit, newdata = testData)

# 计算预测准确度
accuracy <- mean(plsda.pred$class == testData$Species)
cat("PLS-DA accuracy is", round(accuracy, 2))
```


*vegan包的分析*

```{r}
# 进行 PLS-DA 分析
plsda.fit <- rda(scale(iris[, 1:4]), iris$Species, method = "pls", nf = 3)

# 预测测试集
plsda.pred <- predict(plsda.fit, testData[, 1:4], display = "sites")

# 计算预测准确度
accuracy <- mean(plsda.pred$classes == testData$Species)
cat("PLS-DA accuracy is", round(accuracy, 2))

```













