---
title: "nonlinear regression analysis"
format: html
---

## nonlinear regression analysis

> http://www.sthda.com/english/articles/40-regression-analysis/162-nonlinear-regression-essentials-in-r-polynomial-and-spline-regression-models/
> https://statisticsbyjim.com/regression/difference-between-linear-nonlinear-regression-models/
> https://statisticsbyjim.com/regression/choose-linear-nonlinear-regression/


éçº¿æ€§æ¨¡å‹æ˜¯ä¸€ç³»åˆ—æ¨¡å‹çš„ç»Ÿç§°ã€‚å…¶å’Œçº¿æ€§å›å½’åŒºåˆ†çš„é‡ç‚¹åœ¨äºï¼Œå½“ç„¶çº¿æ€§æ¨¡å‹äº¦å¯æ‹Ÿåˆéçº¿æ€§å…³ç³»ï¼Œæ¯”å¦‚å¤šé¡¹å¼å›å½’ã€GAMéƒ½å¯ä»¥å½’å±äºçº¿æ€§æ¨¡å‹ï¼Œä½†å…¶ç”¨äºæ‹Ÿåˆéçº¿æ€§å…³ç³»ã€‚

GLMså¯ä»¥ç†è§£æˆå°†éçº¿æ€§åˆ†å¸ƒçš„outcomeæ•°æ®è¿›è¡Œè½¬æ¢æˆçº¿æ€§è€Œè¿›è¡Œåˆ†æçš„ï¼Œå› å˜é‡å’Œè‡ªå˜é‡é—´çš„çº¿æ€§å…³ç³»ã€‚ç½—è¾‘å›å½’ç»å¸¸è¢«è§†ä¸ºçº¿æ€§å›å½’ã€‚The _linear_ in "generalized linear model" says the parameters enter the model linearly. Due to the parameter ğœ™1
 the logistic growth model is not exactly equal to the logistic function used in logistic regression. å½“ç„¶logisticçš„æ‹Ÿåˆå…¶æœ¬èº«æ˜¯éçº¿æ€§çš„ï¼Œæ˜¯åœ¨ç»è¿‡logitè½¬æ¢åa logit-linear modelï¼Œå…¶çš„å…¬å¼æ‰å‘ˆç°ä¸ºçº¿æ€§çš„æ¨¡å¼ã€‚

æ€ä¹ˆæ›´ä¸ºåˆé€‚çš„åŒºåˆ†ï¼Œçº¿æ€§å’Œéçº¿æ€§å‘¢ï¼Ÿå¤šé¡¹å¼å›å½’åº”è¯¥å±äºçº¿æ€§å›å½’ï¼Œè™½åˆ™å…¶æ‰€æ‹Ÿåˆçš„æ›²çº¿æ˜¯å¼¯æ›²çš„ï¼Œ*linear in the parameters*ï¼Œå¯ä»¥ä½œä¸ºåŒºåˆ†çš„æ ‡å‡†ï¼Œæˆ–è€…è¯´ç¬¦åˆ$y=a1*x1+a2x2+...$æ ¼å¼çš„éƒ½å±äºçº¿æ€§å›å½’ã€‚å…¶ä»–çš„å¯ä»¥ç†è§£ä¸ºéçº¿æ€§å›å½’ã€‚

Like OLS, nonlinear regression estimates the parameters by minimizing the SSE. However, nonlinear models use an iterative algorithm rather than the linear approach of solving them directly with matrix equations. 

The main positive is that nonlinear regression provides the most flexible curve-fitting functionality. The downside is that it can take considerable effort to choose the nonlinear function that creates the best fit for the particular shape of the curve. Unlike linear regression, you also need to supply starting values for the nonlinear algorithm. 

ä¸‹é¢æ•´ç†ä¸€äº›å¸¸è§çš„éçº¿æ€§æ¨¡å‹çš„ä¾‹å­ï¼š

1. Michaelis-Menten Regression model(rectangular hyperbola)
2. Generalized Additive Models(ä¼¼ä¹åº”è¯¥å±äºçº¿æ€§)
3. loess (package â€˜statsâ€™)
4. quantile regression (packages 'quantreg')
5. segmented regression (segmented in package 'segmented')
6. power
7. Weibull growth, Fourier, log-logisticç­‰ã€‚


*éçº¿æ€§å›å½’ä¸­çš„convergence failsé—®é¢˜ï¼š*
Convergence problems in nonlinear models can be caused by many different reasons. These are a few of them:

The model is not appropriate for the observed data (or vice versa)
The model is conceptually correct but there is an error in the formula
The model is too complex; a simpler model should be used
The model is too simple; a more complex model should be used
Starting values are too far from the solution


Nonlinear regression models are generally assumed to be *parametric*, where the model is described as a nonlinear equation. Typically machine learning methods are used for non-parametric nonlinear regression.

*Popular algorithms for fitting a nonlinear regression include:*
- Gauss-Newton algorithm
- Gradient descent algorithm
- Levenberg-Marquardt algorithm


Itâ€™s important to note that R-squared is invalid for nonlinear models and statistical software *canâ€™t calculate p-values for the terms/estimates*.ä¸è¿‡åœ¨Prismç­‰è½¯ä»¶ä¸­ï¼Œåœ¨æ‹Ÿåˆdose-response curveæ—¶ï¼Œä¼šç»™å‡ºR2å€¼ã€‚
è¿™æ˜¯éœ€è¦æ³¨æ„çš„åœ°æ–¹ï¼Œç”¨è¿™äº›R2åšåˆ¤æ–­å®¹æ˜“å‡ºç°é—®é¢˜ï¼Œsome like:
R-squared is consistently high for both excellent and appalling models.
R-squared will not rise for better models all of the time.
If you use R-squared to pick the best model, it leads to the proper model only 28-43% of the time.


*å¯ç”¨æ¥è¯„ä»·éçº¿å½¢æ¨¡å‹çš„æŒ‡æ ‡ï¼š*
There are other goodness-of-fit measures you can use for regression models that are not linear. For instance, you can use the standard error of the regression. For this statistic, smaller values represent better models.


```{r}
library(nlraa)
library(minpack.lm)
library(nls2)
```



### base R

`stats::nls()`å‡½æ•°ä¸­start

```{r}
stats::nls()
```


### Polynomial regression

```{r}
data("Boston", package = "MASS")

training.samples <- Boston$medv %>%
  caret::createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]


lm(medv ~ lstat + I(lstat^2), data = train.data)
lm(medv ~ poly(lstat, 2, raw = TRUE), data = train.data)



```


```{r}
ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = lm, formula = y ~ poly(x, 5, raw = TRUE))
```


*Log transformation*

```{r}
# Build the model
model <- lm(medv ~ log(lstat), data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv)
)


ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = lm, formula = y ~ log(x))
```


### Spline regression



```{r}
library(splines)
# Build the model
knots <- quantile(train.data$lstat, p = c(0.25, 0.5, 0.75))
model <- lm (medv ~ bs(lstat, knots = knots), data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv)
)
```



### self-starting functions

```{r}
library(nlraa)
```


ä¸åŒäº`stats::nls()`ä¸­çš„startå‚æ•°éœ€è¦è‡ªå·±æä¾›, `stats`åŒ…è¿˜æœ‰

1. Michaelis-Menten Model(SSmicmen)
2. Asymptotic Regression Model (SSasymp)
3. Four Parameter Logistic Model (SSfpl)
4. Self-Starting First-Order Compartment Function (SSfol)
5. Self-Starting Weibull Growth Function (SSweibull)
6. SSlogis (Logistic)
7. SSasympOff (Asymptotic with an offset)
8. SSasympOrig (Asymptotic through the Origin)
9. SSbiexp (Bi-exponential)
10. SSfpl (Four parameter logistic)


```{r}

```




















