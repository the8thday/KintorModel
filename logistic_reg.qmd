---
title: "logistic regression"
format: html
date: 2023-05-18
---

## é‡æ–°æ•´ç†logisticå›å½’çš„ç¬”è®°

> https://cscu.cornell.edu/wp-content/uploads/91_ordlogistic.pdf
> https://cscu.cornell.edu/workshop/logisic_regression_analysis
> https://stats.oarc.ucla.edu/r/faq/ologit-coefficients/
> https://stats.oarc.ucla.edu/r/dae/ordinal-logistic-regression/
> https://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/
> https://bookdown.org/chua/ber642_advanced_regression/
> https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html


åœ¨æ­¤å¤„é‡æ–°è®°å½•ä¸€ä¸‹ç½—è¾‘å›å½’çš„ç›¸å…³å†…å®¹ã€‚


é¦–å…ˆæ˜¯logisticåˆ†å¸ƒï¼Œé€»è¾‘åˆ†å¸ƒçš„CDFåœ¨ç”Ÿå­˜åˆ†ææ—¶å·²ç»æœ‰æ‰€äº†è§£ã€‚åœ¨åˆ†ææ¦‚ç‡é—®é¢˜æ—¶ï¼Œå¯ä»¥å°†å…¶ç†è§£ä¸ºä¸¤å‚æ•°æ¨¡å‹ï¼Œæœ€å¤§å’Œæœ€å°å€¼åˆ†åˆ«ä¸º1å’Œ0ã€‚

odds: $odd=p/(1-p)$
logit: $logit(p)=log(odds)$

é€»è¾‘å›å½’å‡è®¾æ•°æ®æœä»ä¼¯åŠªåˆ©åˆ†å¸ƒï¼ˆå› å˜é‡ï¼‰ï¼Œé€šè¿‡æå¤§åŒ–ä¼¼ç„¶å‡½æ•°çš„æ–¹æ³•ï¼Œè¿ç”¨æ¢¯åº¦ä¸‹é™æ¥æ±‚è§£å‚æ•°ï¼Œæ¥è¾¾åˆ°äºŒåˆ†ç±»çš„ç›®çš„ã€‚åœ¨


*Statistical Software: What You Need to Know: *There are several ways in which an ordinal regression model can be parameterized and different statistical software packages use different parameterizations. Thus, great care should be taken when interpreting the output from `ordinal regression models`.




```{r, include=FALSE}
library(tidyverse)
library(easystats)
library(MASS)
library(car)
```


### å±é™©å› ç´ åˆ†æï¼Œæˆ–è€…å˜é‡é—´å…³ç³»åˆ†æ

å’Œä½œä¸ºé¢„æµ‹æ¨¡å‹ç¨æœ‰ä¸åŒï¼Œä½œä¸ºè€ƒå¯Ÿå˜é‡é—´å…³ç³»çš„æ¨¡å‹ï¼Œå…¶æœ‰ç‹¬ç‰¹çš„ç»Ÿè®¡åˆ†ææ€è·¯ã€‚


#### äºŒåˆ†ç±»é—®é¢˜

æœ€ä¸ºå¸¸è§„çš„ä½¿ç”¨æ˜¯å¤„ç†äºŒåˆ†ç±»é—®é¢˜ã€‚

```{r}
data(bivariate, package = 'modeldata')

head(bivariate_train)
```

```{r}
fit1 <- stats::glm(Class ~ ., data = bivariate_train, family = binomial(link = 'logit'))

summary(fit1)
```


```{r}
# å’ŒSPSSä»¥åŠJMPåšå¯¹æ¯”
dat <- haven::read_spss('~/Downloads/äºŒåˆ†ç±»Logisticå›å½’ã€ç®€ã€‘-SPSSæ•™ç¨‹-åŒ»å’–ä¼š/Logisticå›å½’.sav') %>% 
  labelled::unlabelled()

dat

fit1 <- glm(cancer ~ sex + age + BMI + COPD + smoke, data = dat,
            family = binomial(link = 'logit')
            )

summary(fit1)
```

```{r}
## odds ratios and 95% CI
## CIs using profiled log-likelihood
exp(cbind(OR = coef(fit1), confint(fit1)))
```
*é‚£ä¹ˆä¾æ®è¿™äº›ç»“æœæ€ä¹ˆåˆ†æå±é™©å› ç´ å‘¢ï¼š*é¦–å…ˆæ˜¯A/Bå˜é‡çš„ORå€¼ï¼Œper unit change in regressor.
Note that while R produces it, the odds ratio for the intercept is not generally interpreted.



*We can test for an overall effect of rank using the wald.test function of the aod library. *
```{r}
# Run a wald.test
# ä¼¼ä¹ä¸€æ¬¡åªèƒ½runä¸€ä¸ªtermï¼Œæ€ä¹ˆrunå…¨éƒ¨å‘¢
aod::wald.test(b=coef(fit1),Sigma=vcov(fit1), Terms = 3)

# car::Anova å°±å¯ä»¥
```
The chi-squared test statistic of 20.9, with three degrees of freedom is associated with a p-value of 0.00011 indicating that the overall effect of rank is statistically significant.


*Likelihood Ratio Tests: *
```{r}
# Whole model test
lmtest::lrtest(fit1)

# logLik(fit1)

# Effect Likelihood Ratio Test
# for every term
# same as SAS/JMP
car::Anova(fit1, type = 'III', test.statistic='LR')
```
The likelihood ratio test is based on -2LL ratio. It is a test of the significance of the difference between the likelihood ratio (-2LL) for the researcherâ€™s model with predictors (called model chi square) minus the likelihood ratio for baseline model with only a constant in it.


*anova, table of deviance*
```{r}
anova(fit1, test="Chisq")

car::Anova(fit1, type='III',test.statistic='Wald')
```
*anova*å¾—åˆ°çš„æ¯ä¸ªå˜é‡çš„På€¼ï¼Œå…¶å¯ä»¥è§£é‡Šä¸ºå’Œnull modelç›¸æ¯”Devianceå€¼è¶Šå¤§è¶Šå¥½ï¼ŒThe difference between the null deviance and the residual deviance shows how our model is doing against the null model (a model with only the intercept). The wider this gap, the better. Analyzing the table we can see the drop in deviance when adding each variable one at a time. Again, adding B significantly reduces the residual deviance. A large p-value here indicates that the model without the variable explains more or less the same amount of variation.


```{r}
# Boxâ€“Tidwell
car::boxTidwell(fit1)
```


*easystatsç³»åˆ—æä¾›çš„ä¸€ç³»åˆ—æ–¹ä¾¿çš„å‡½æ•°ï¼š*
```{r}
parameters(fit1, exponentiate = TRUE)
```
```{r}
performance::performance(fit1)

pscl::pR2(fit1)
```
å’Œçº¿æ€§å›å½’ä¸åŒï¼Œå¯¹äºé€»è¾‘å›å½’æ²¡æœ‰ä¸€ä¸ªç¡®åˆ‡çš„R2ï¼Œå¯ä»¥é‡‡ç”¨Tjur's R2æˆ–è€…McFadden R2è¿›è¡Œé˜é‡Šã€‚



*æ¨¡å‹è¯„ä»·å’Œé¢„æµ‹ï¼š*
```{r}
# ?predict.glm
# é»˜è®¤ä¸ºlinkï¼Œlog-odds (probabilities on logit scale) 
predict(fit1) |> head()

# è¿”å›çš„ä¸ºprobabilities in the form of P(y=1|X). 
# Our decision boundary will be 0.5. If P(y=1|X) > 0.5 then y = 1 otherwise y=0. 
p <- predict(fit1, type = 'response') 
p |> head()
```


```{r}
broom::augment(fit1,type.predict='response')
```


*é©¬ä¿®æ–¯ç›¸å…³ç³»æ•°, matthews correlation coefficient(phi coefficient)*
```{r}
Logistic_Predictions = predict(fit1, type = 'response')
Logistic_Predictions_binomial=ifelse(Logistic_Predictions>0.5,1,0)
con_Logistic <- caret::confusionMatrix(factor(Logistic_Predictions_binomial), factor(dat$cancer))

yardstick::mcc(con_Logistic$table)
```



*å˜é‡é‡è¦æ€§ï¼š*
```{r}
vip::vi(fit1)

caret::varImp(fit1)
```
Higher values indicate more importance.



*ç»˜åˆ¶å¯¹åº”çš„å›¾è¡¨ï¼š*
```{r}
gtsummary::tbl_regression(fit1, exponentiate = TRUE)
```


*ROC*
```{r}
# æ„Ÿè§‰è¿™ä¸ªåŒ…ä¸å¦‚pROC
require(ROCR)


pr <- prediction(p, dat$cancer, label.ordering=c('éè‚ºç™Œ', 'è‚ºç™Œ'))
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```

*ROC plot by pROC*
```{r}
# library(pROC)

roc1 <- pROC::roc(dat$cancer, p,
  percent = TRUE,
  # arguments for auc
  # partial.auc = c(100, 90), partial.auc.correct = TRUE,
  # partial.auc.focus = "sens",
  # arguments for ci
  ci = TRUE, boot.n = 100, ci.alpha = 0.9, stratified = FALSE,
  # arguments for plot
  plot = TRUE, auc.polygon = TRUE, max.auc.polygon = TRUE, grid = TRUE,
  print.auc = TRUE, show.thres = TRUE
)
```
```{r}
coords(roc1, "best", ret=c("threshold", "specificity", "1-npv"))
coords(roc1, "local maximas", ret=c("threshold", "sens", "spec", "ppv", "npv"))
```



#### Multinomial Logistic Regression

é™¤äºŒåˆ†ç±»é—®é¢˜å¤–ï¼Œlogisticå¯ä»¥æ‹Ÿåˆå¤šåˆ†ç±»é—®é¢˜ï¼Œæ­¤å¤„è®°å½•ä¸ºæ— åºå¤šåˆ†ç±»é—®é¢˜çš„ç¬”è®°ã€‚

å¯¹äºå¤šåˆ†ç±»é—®é¢˜ï¼Œå¸¸ç”¨çš„æ–¹æ³•è¿˜åŒ…æ‹¬`log-linear`, `Discriminant Analysis`ç­‰ï¼Œå½“ç„¶è¿˜æœ‰å¤šç§æœºå™¨å­¦ä¹ çš„æ–¹æ³•å¯ä¾›é€‰æ‹©ã€‚
å…¶ä¸­`log-linear`çš„è¯¦ç»†ç¬”è®°è§`log_linear_model.qmd`æ–‡æ¡£ã€‚åˆ¤åˆ«åˆ†æçš„è„šæœ¬è§`CA_DA.Rmd`ã€‚


```{r}
require(nnet)
```


åœ¨æ— åºå¤šåˆ†ç±»é€»è¾‘å›å½’ä¸­ï¼Œéœ€è¦æ³¨æ„å¯¹äºå› å˜é‡referenceçš„è®¾ç½®ï¼Œä¹Ÿå°±æ˜¯ä½ æ¬²åšå¯¹æ¯”çš„é‚£ä¸ªlevelã€‚
```{r}
# å‡è®¾æ­¤æ•°æ®é›†ä¸ºæ— åºå¤šåˆ†ç±»ï¼Œè™½åˆ™å…¶å®ä¸ºæœ‰åºå¤šåˆ†ç±»ã€‚
d <- haven::read_sav('~/Downloads/æœ‰åºå¤šåˆ†ç±»Logisticå›å½’ã€è¯¦ã€‘-SPSSæ•™ç¨‹-åŒ»å’–ä¼š/26 æœ‰åºLogisticå›å½’.sav') %>% 
  labelled::unlabelled()

glimpse(d)
```

```{r}
fit_norminal <- nnet::multinom(formula = patient_satisfaction ~ ., data = d, model = TRUE)

summary(fit_norminal)
```

```{r}
# extract the coefficients from the model and exponentiate
exp(coef(fit_norminal))
```

```{r}
# JMP or SAS produce LR & Wald, here we only show the LR
car::Anova(fit_norminal, type = 'III')
```

*Calculate the Goodness of fit: *
```{r}
# chiseq å¯¹äºå¤šåˆ†ç±»é—®é¢˜ä¸æ˜¯å¾ˆåˆé€‚
chisq.test(d$patient_satisfaction,predict(fit_norminal))
```

Calculate the Pseudo R-Square
```{r}
# æœ‰ä¸€ä¸ªå¾ˆç¥å¥‡çš„æŠ¥é”™ï¼Œnnet::multinomï¼Œä¸è¯†åˆ«è¿™ç§æ ¼å¼ã€‚ã€‚
DescTools::PseudoR2(fit_norminal, which = c("CoxSnell","Nagelkerke","McFadden"))
```
Interpretation of the R-Square:

These are three pseudo R squared values. Logistic regression does not have an equivalent to the R squared that is found in OLS regression; however, many people have tried to come up with one. These statistics do not mean exactly what R squared means in OLS regression (the proportion of variance of the response variable explained by the predictors), we suggest interpreting them with great caution.

Cox and Snellâ€™s R-Square imitates multiple R-Square based on â€˜likelihoodâ€™, but its maximum can be (and usually is) less than 1.0, making it difficult to interpret. Here it is indicating that there is the relationship of 31% between the dependent variable and the independent variables. Or it is indicating that 31% of the variation in the dependent variable is explained by the logistic model.

The Nagelkerke modification that does range from 0 to 1 is a more reliable measure of the relationship. Nagelkerkeâ€™s R2 will normally be higher than the Cox and Snell measure. In our case it is 0.357, indicating a relationship of 35.7% between the predictors and the prediction.

McFadden = {LL(null) â€“ LL(full)} / LL(null). In our case it is 0.182, indicating a relationship of 18.2% between the predictors and the prediction.


*Likelihood Ratio Tests*
Whole model test: æ­¤å¤„çš„ç»“æœå³æ˜¯JMP/SPSSä¸­
```{r}
lmtest::lrtest(fit_norminal)
```

```{r}
lmtest::lrtest(fit_norminal, 'gender')
```
The results of the likelihood ratio tests can be used to ascertain the significance of predictors to the model.
These likelihood statistics can be seen as sorts of overall statistics that tell us which predictors significantly enable us to predict the outcome category, but they donâ€™t really tell us specifically what the effect is. To see this we have to look at the individual parameter estimates.


```{r}
# we can also use the performance package
performance::performance(fit_norminal)
```

*calculate p-value based on z-value*
```{r}
# Calculate z-values
zvalues <- summary(fit_norminal)$coefficients / summary(fit_norminal)$standard.errors

# Remember that the normal distribution doesnâ€™t depend on degrees of freedom.
pnorm(abs(zvalues), lower.tail=FALSE)*2
# 2-tailed z test
# (1 - pnorm(abs(zvalues), 0, 1)) * 2
```



*Build a classification table*
```{r}
# Load the summarytools package to use the classification function
# library(summarytools)
# Build a classification table by using the ctable function
ctable <- table(d$patient_satisfaction, predict(fit_norminal))
ctable
```


*ROCæ›²çº¿ï¼š*

åœ¨å¤šåˆ†ç±»é—®é¢˜ä¸­ï¼ŒTPRå’ŒFPRéœ€è¦å°†é—®é¢˜è½¬ä¸ºäºŒåˆ†ç±»é—®é¢˜åå†è¿›è¡Œè®¡ç®—ã€‚è½¬æ¢çš„æ–¹å¼æœ‰ä¸¤ç§ï¼š
- the One-vs-Rest scheme compares each class against all the others (assumed as one);
- the One-vs-One scheme compares every unique pairwise combination of classes.

Accuracy and Kappa use the same definitions as their binary counterpart, Matthews correlation coefficient (MCC) has a known multiclass generalization as well, sometimes called the ğ‘…ğ¾ statistic.

`Macro averaging`, reduces your multiclass predictions down to multiple sets of binary predictions, calculates the corresponding metric for each of the binary cases, and then averages the results together. macro averaging reduces the problem to multiple one-vs-all comparisons. 
`Micro averaging` treats the entire set of data as an aggregate result, and calculates 1 metric rather than k metrics that get averaged together.


*One-vs-One*

```{r}
library(pROC)

# same as fitted(fit_norminal)
p <- predict(fit_norminal, type = 'probs')

roc.multi <- pROC:::multiclass.roc(d$patient_satisfaction, p,
  plot = TRUE, auc.polygon = TRUE, max.auc.polygon = TRUE, grid = TRUE,
  print.auc = TRUE, show.thres = TRUE, 
  # add = TRUE
)

auc(roc.multi)
```


```{r}
# individual roc curve info for each classes
# ä¹Ÿå°±æ˜¯å››ä¸ªåˆ†ç»„ä¸¤ä¸¤ç»„å’Œï¼Œä¸èƒ½é‡å¤çš„æ’åˆ—ï¼Œå…±æœ‰6ç§ç»„åˆï¼Œå…¶ä¸­æ¯ä¸ªç»„åˆåˆæœ‰ä¸¤ä¸ªcontrolå’Œtestç»„ç›¸åçš„æƒ…å†µï¼Œå…±12ç§æƒ…å†µ
roc.multi['rocs'] |> lengths()
```


```{r}
pROC:::multiclass.roc(d$patient_satisfaction, p, levels=c('ä¸æ»¡æ„', 'éå¸¸æ»¡æ„'), 
                      plot=T,
                      direction='>',
                      print.auc = TRUE, show.thres = TRUE
                      )
```

```{r}
# plot 6ç§ç»„åˆä¸­çš„ç¬¬ä¸€ç§
rs <- roc.multi[['rocs']]
plot.roc(rs[[1]][[1]])
sapply(2:length(rs),function(i) lines.roc(rs[[i]][[1]],col=i))
```


*One-vs-Rest multiclass ROC*

In each step, a given class is regarded as the positive class and the remaining classes are regarded as the negative class as a bulk.
```{r}
# åœ¨Ré‡Œé¢å®ç°èµ·æ¥ä¼¼ä¹è¿˜æœ‰äº›å›°éš¾
# ä¼¼ä¹è¦fitå¥½å‡ æ¬¡

library(multiROC)
# å‘ç°äº†ä¸€ä¸ªRåŒ…ï¼Œè™½ç„¶ç”¨èµ·æ¥ä¸æ˜¯å¾ˆæ–¹ä¾¿
```

```{r}
p1 <- as.data.frame(p)
p1$patient_satisfaction <- d$patient_satisfaction

p2 <- fastDummies::dummy_cols(p1, select_columns = 'patient_satisfaction') %>% 
  dplyr::select(starts_with('patient_satisfaction'), everything()) %>% 
  dplyr::select(-patient_satisfaction) %>% 
  dplyr::rename('ä¸æ»¡æ„_true'='patient_satisfaction_ä¸æ»¡æ„',
                'ä¸€èˆ¬_true'='patient_satisfaction_ä¸€èˆ¬',
                'æ»¡æ„_true'='patient_satisfaction_æ»¡æ„',
                'éå¸¸æ»¡æ„_true'='patient_satisfaction_éå¸¸æ»¡æ„',
                'ä¸æ»¡æ„_pred_multinom'='ä¸æ»¡æ„',
                'ä¸€èˆ¬_pred_multinom'='ä¸€èˆ¬',
                'æ»¡æ„_pred_multinom'='æ»¡æ„',
                'éå¸¸æ»¡æ„_pred_multinom'='éå¸¸æ»¡æ„'
                )

res <- multiROC::multi_roc(p2, force_diag=T)

# same as JMP
unlist(res$AUC)
```

```{r}
n_method <- length(unique(res$Methods))
n_group <- length(unique(res$Groups))
res_df <- data.frame(Specificity = numeric(0), Sensitivity = numeric(0), 
                     Group = character(0), AUC = numeric(0), Method = character(0))
for (i in 1:n_method) {
  for (j in 1:n_group) {
    temp_data_1 <- data.frame(
      Specificity = res$Specificity[[i]][j],
      Sensitivity = res$Sensitivity[[i]][j],
      Group = unique(res$Groups)[j],
      AUC = res$AUC[[i]][j],
      Method = unique(res$Methods)[i]
    )
    colnames(temp_data_1) <- c("Specificity", "Sensitivity", "Group", "AUC", "Method")
    res_df <- rbind(res_df, temp_data_1)
  }
  temp_data_2 <- data.frame(
    Specificity = res$Specificity[[i]][n_group + 1],
    Sensitivity = res$Sensitivity[[i]][n_group + 1],
    Group = "Macro",
    AUC = res$AUC[[i]][n_group + 1],
    Method = unique(res$Methods)[i]
  )
  temp_data_3 <- data.frame(
    Specificity = res$Specificity[[i]][n_group + 2],
    Sensitivity = res$Sensitivity[[i]][n_group + 2],
    Group = "Micro",
    AUC = res$AUC[[i]][n_group + 2],
    Method = unique(res$Methods)[i]
  )
  colnames(temp_data_2) <- c("Specificity", "Sensitivity", "Group", "AUC", "Method")
  colnames(temp_data_3) <- c("Specificity", "Sensitivity", "Group", "AUC", "Method")
  res_df <- rbind(res_df, temp_data_2)
  res_df <- rbind(res_df, temp_data_3)
}

ggplot2::ggplot(res_df, ggplot2::aes(x = 1 - Specificity, y = Sensitivity)) +
  ggplot2::geom_path(ggplot2::aes(color = Group, linetype = Method)) +
  ggplot2::geom_segment(ggplot2::aes(x = 0, y = 0, xend = 1, yend = 1), colour = "grey", linetype = "dotdash") +
  ggplot2::theme_bw() +
  ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5), legend.justification = c(1, 0), 
                 legend.position = c(.95, .05), legend.title = ggplot2::element_blank(), 
                 legend.background = ggplot2::element_rect(fill = NULL, size = 0.5, linetype = "solid", 
                                                           colour = "black"))
```


```{r}
library(yardstick)
```

```{r}
p1$predict <- predict(fit_norminal, type = 'class')

# # Macro averaged multiclass precision
yardstick::accuracy(p1, patient_satisfaction, predict)
```
```{r}
# method from Hand, Till, (2001). 
yardstick::roc_auc(data = p1,
                   truth = patient_satisfaction,
                   `ä¸æ»¡æ„`:`éå¸¸æ»¡æ„`
                   )

# å¦‚ä½•å¾—åˆ°æ¯ä¸€ä¸ªåˆ†ç±»çš„aucï¼Œå³ä¸‹å›¾ä¸­AUCæ›²çº¿ä¸‹çš„é¢ç§¯

```
```{r}
# If a multiclass truth column is provided, a one-vs-all approach will be taken to calculate multiple curves, one per level.
# p1 %>% roc_curve(truth = patient_satisfaction,
#                  `ä¸æ»¡æ„`:`éå¸¸æ»¡æ„`
#                  ) %>% 
#   autoplot()

# a much more beautiful plot
roc_curve(p1, patient_satisfaction, `ä¸æ»¡æ„`:`éå¸¸æ»¡æ„`) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = .level)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw()
```

```{r}
yardstick::lift_curve(p1, truth = patient_satisfaction,
                      `ä¸æ»¡æ„`:`éå¸¸æ»¡æ„`
                      ) %>% 
  autoplot()
```



#### æœ‰åºå¤šåˆ†ç±»é—®é¢˜

ä½¿ç”¨æœ‰åºLogisticè¿›è¡Œå›å½’åˆ†ææ—¶ï¼Œéœ€è¦è€ƒè™‘4ä¸ªå‡è®¾ã€‚ 
- å‡è®¾1ï¼šå› å˜é‡å”¯ä¸€ï¼Œä¸”ä¸ºæœ‰åºå¤šåˆ†ç±»å˜é‡ï¼Œå¦‚è¡€å‹æ°´å¹³å¯ä»¥åˆ†ä¸ºé«˜ã€ä¸­ã€ä½ï¼›æŸç—…çš„æ²»ç–—æ•ˆæœåˆ†ä¸ºç—Šæ„ˆã€æœ‰æ•ˆã€æ— æ•ˆç­‰ã€‚
- å‡è®¾2ï¼šå­˜åœ¨ä¸€ä¸ªæˆ–å¤šä¸ªè‡ªå˜é‡ï¼Œå¯ä¸ºè¿ç»­ã€æœ‰åºå¤šåˆ†ç±»æˆ–æ— åºåˆ†ç±»å˜é‡ã€‚
- å‡è®¾3ï¼šè‡ªå˜é‡ä¹‹é—´æ— å¤šé‡å…±çº¿æ€§ã€‚
- å‡è®¾4ï¼šæ¨¡å‹æ»¡è¶³æ¯”ä¾‹ä¼˜åŠ¿å‡è®¾ã€‚æ„æ€æ˜¯æ— è®ºå› å˜é‡çš„åˆ†å‰²ç‚¹åœ¨ä»€ä¹ˆä½ç½®ï¼Œæ¨¡å‹ä¸­å„ä¸ªè‡ªå˜é‡å¯¹å› å˜é‡çš„å½±å“ä¸å˜ï¼Œä¹Ÿå°±æ˜¯è‡ªå˜é‡å¯¹å› å˜é‡çš„å›å½’ç³»æ•°ä¸åˆ†å‰²ç‚¹æ— å…³ã€‚æœ‰åºå¤šåˆ†ç±»çš„Logisticå›å½’åŸç†æ˜¯å°†å› å˜é‡çš„å¤šä¸ªåˆ†ç±»ä¾æ¬¡åˆ†å‰²ä¸ºå¤šä¸ªäºŒå…ƒçš„Logisticå›å½’ï¼Œä¾‹å¦‚æœ¬ä¾‹ä¸­å› å˜é‡æ‚£è€…æ»¡æ„åº¦æœ‰4ä¸ªç­‰çº§ï¼Œåˆ†ææ—¶æ‹†åˆ†ä¸ºä¸‰ä¸ªäºŒå…ƒLogisticå›å½’ï¼Œåˆ†åˆ«ä¸º(0 vs 1+2+3) ã€(0+1 vs 2+3)å’Œ(0+1+2 vs 3)ï¼Œå‡æ˜¯è¾ƒä½çº§ä¸è¾ƒé«˜çº§å¯¹æ¯”ã€‚åœ¨æœ‰åºå¤šåˆ†ç±»Logisticå›å½’ä¸­ï¼Œå‡è®¾å‡ ä¸ªäºŒå…ƒLogisticå›å½’çš„è‡ªå˜é‡ç³»æ•°ç›¸ç­‰ï¼Œä»…å¸¸æ•°é¡¹ä¸ç­‰ï¼Œç»“æœä¹Ÿåªè¾“å‡ºä¸€ç»„è‡ªå˜é‡çš„ç³»æ•°ã€‚å› æ­¤ï¼Œæœ‰åºå¤šåˆ†ç±»çš„Logisticå›å½’æ¨¡å‹ï¼Œå¿…é¡»å¯¹è‡ªå˜é‡ç³»æ•°ç›¸ç­‰çš„å‡è®¾ï¼ˆå³æ¯”ä¾‹ä¼˜åŠ¿å‡è®¾ï¼‰è¿›è¡Œæ£€éªŒï¼ˆåˆç§°å¹³è¡Œçº¿æ£€éªŒï¼‰ã€‚å¦‚æœä¸æ»¡è¶³è¯¥å‡è®¾ï¼Œåˆ™è€ƒè™‘ä½¿ç”¨æ— åºå¤šåˆ†ç±»Logisticå›å½’ã€‚


è™½ç„¶åˆ©ç”¨è½¯ä»¶åšæœ‰åºé€»è¾‘å›å½’è¿˜æ˜¯è¾ƒä¸ºç®€å•çš„ï¼Œä½†æ˜¯interpretç»“æœå°±æ˜¾å¾—æœ‰äº›éº»çƒ¦äº†ï¼Œéå¸¸å€¼å¾—åšä¸€ä¸ªè®°å½•ç¬”è®°ã€‚
A cumulative logit parameterization is used in ordinal logistic regression models. æ‰€ä»¥åœ¨ç»“æœä¸­å¯ä»¥çœ‹åˆ°å¯¹äºé™¤referenceå¤–
çš„å…¨éƒ¨leveléƒ½ç»™å‡ºäº†ä¸€ä¸ªEstimateå€¼ã€‚æ¯”å¦‚SPSSé‡Œçš„Thresholdå€¼ã€‚


*ä¸åŒè½¯ä»¶ç»™å‡ºçš„ç»“æœçš„ä¸åŒï¼š*SPSSå’Œ`MASS::polr`é‡‡ç”¨ç¬¬ä¸€ç§parameterizationsæ–¹æ³•ï¼Œå…¶ç»™å‡ºçš„å‚æ•°ä¸ºæ­£æ•°ï¼Œåœ¨ä¹¦å†™å…¬å¼æ—¶éœ€è¦åŠ ä¸Š-è´Ÿå·ã€‚JMPå’ŒSASé‡‡ç”¨çš„æ˜¯ç¬¬äºŒç§æ–¹æ³•ï¼Œå¯ä»¥çœ‹åˆ°å…¶ç»™å‡ºçš„å‚æ•°å¸¦æœ‰è´Ÿå·ï¼ŒåŒæ—¶JMPçš„åˆ†ç±»å˜é‡é‡‡ç”¨Effectè€Œä¸æ˜¯Dummyã€‚Rä¸­çš„`VGAM::vglm`,`rms::lrm`åˆ†åˆ«å¯ä»¥å®ç°ç¬¬äºŒç§å’Œç¬¬ä¸‰ç§å‚æ•°åŒ–æ–¹æ³•ï¼Œå…·ä½“çš„å†…å®¹è§å‚è€ƒé“¾æ¥1.
R, Stata, SPSS, and SAS (using proc genmod) use dummy coding, while JMP and SAS (using proc logistic) use effect coding (see Statnews #72 for more information on these two coding schemes). Both R and Stata use the first level alphanumerically as the reference level, whereas SAS, JMP, and SPSS use the last level as the reference level.
However, it is possible to customize the reference level in each of these programs.
JMPå¯¹äºåˆ†ç±»å˜é‡é‡‡ç”¨çš„ç¼–ç æ–¹å¼å’ŒSPSSä»¥åŠRä¸åŒã€‚


```{r}
library(ordinal)
```


```{r}
library(labelled)

dd <- haven::read_sav('~/Downloads/æœ‰åºå¤šåˆ†ç±»Logisticå›å½’ã€è¯¦ã€‘-SPSSæ•™ç¨‹-åŒ»å’–ä¼š/26 æœ‰åºLogisticå›å½’.sav') %>% 
  labelled::unlabelled()

glimpse(dd)

dd <- dd %>% mutate(gender = factor(gender, levels = c('å¥³æ€§','ç”·æ€§')),
                    treatment = fct_rev(treatment),
                    patient_satisfaction = factor(patient_satisfaction,
                                                  levels = c('ä¸æ»¡æ„','ä¸€èˆ¬','æ»¡æ„','éå¸¸æ»¡æ„'), 
                                                  ordered = TRUE
                                                  )
                    )

paint::paint(dd)
```


```{r}
# The command name comes from proportional odds logistic regression, highlighting the proportional odds assumption in our model. 
fit_ordinal <- MASS::polr(formula = patient_satisfaction ~ ., data = dd, Hess = T)

# same as SPSS
summary(fit_ordinal)
```
*interpret: *Call, this is R reminding us what type of model we ran, what options we specified, etc.
Next we see the usual regression output coefficient table including the value of each coefficient, standard errors, and t value, which is simply the ratio of the coefficient to its standard error. There is no significance test by default.
Next we see the estimates for the two intercepts, which are sometimes called cutpoints. The intercepts indicate where the latent variable is cut to make the three groups that we observe in our data. Note that this latent variable is continuous. In general, these are not used in the interpretation of the results. The cutpoints are closely related to thresholds, which are reported by other statistical packages.
Finally, we see the residual deviance, -2 * Log Likelihood of the model as well as the AIC. Both the deviance and AIC are useful for model comparison.


ç”·æ€§è®¤ä¸ºâ€œæ²»ç–—æ»¡æ„åº¦é«˜â€çš„ORå€¼æ˜¯å¥³æ€§çš„1.976å€ï¼ˆ95%CIï¼š1.112-3.511ï¼‰ï¼ŒÏ‡2=5.389ï¼ŒP=0.020ã€‚

å¾—åˆ°ä¸Šè¿°ç»“æœåï¼Œä¾æ®SPSSä½¿ç”¨çš„æ¨¡å‹ï¼Œå¯ä»¥å¾—åˆ°ä¸‰ä¸ªæ–¹ç¨‹ã€‚
Lnï¼ˆpatient_satisfaction=0ï¼‰= 9.175-0.231*age-0.007*fee-0.681genderç”·æ€§-0.032*treatmentdrug1-1.142* treatmentdrug2
Lnï¼ˆpatient_satisfaction=1ï¼‰= 10.998-0.231*age-0.007*fee-0.681genderç”·æ€§-0.032*treatmentdrug1-1.142* treatmentdrug2
Lnï¼ˆpatient_satisfaction=2ï¼‰= 13.984-0.231*age-0.007*fee-0.681genderç”·æ€§-0.032*treatmentdrug1-1.142* treatmentdrug2
å¯ä»¥çœ‹åˆ°ï¼ŒSPSSå¾—åˆ°çš„æ–¹ç¨‹ä¸­ï¼Œé™¤äº†æˆªè·é¡¹ä¹‹å¤–ï¼Œæ‰€æœ‰æ•ˆåº”å€¼è¦åœ¨Parameter Estimatesè¡¨æ ¼ä¸­çš„åŸå§‹å€¼åŸºç¡€ä¸ŠåŠ ä¸Šè´Ÿå·ã€‚


```{r}
(ci <- confint(fit_ordinal)) # default method gives profiled CIs
```

```{r}
## OR and CI
exp(cbind(OR = coef(fit_ordinal), ci))
```
These coefficients are called proportional odds ratios and we would interpret these pretty much as we would odds ratios from a binary logistic regression.


```{r}
car::Anova(fit_ordinal, type='III')
```

*goodness of fit*
```{r}
chisq.test(dd$patient_satisfaction, predict(fit_ordinal))
```


```{r}
lmtest::lrtest(fit_ordinal)
```


*Calculating the Pseudo R-Square*
Measuring Strength of Association
```{r}
DescTools::PseudoR2(fit_ordinal, which = c("CoxSnell","Nagelkerke","McFadden"))
```

They are not as useful as the statistic in multiple regression, since their interpretation is not straightforward.


*test proportional odds assumption*
```{r}
# Load the library
library(brant)
# Run the Brant test on the model
brant(fit_ordinal)
```
Each of the probabilities refers to the null hypothesis that the parallel regression assumption holds, or that the value of the coefficients donâ€™t differ across different cutpoints in the outcome variable. 


*ROC curve:*
è¿™é‡Œæ¶‰åŠåˆ°å’Œæ— åºå¤šåˆ†ç±»ä¸€æ ·çš„é—®é¢˜ï¼Œæ¨èç›´æ¥ä½¿ç”¨`rms` package.
```{r}
library(pROC)

p <- predict(fit_ordinal, type = 'probs')

roc.ordi <- pROC:::multiclass.roc(dd$patient_satisfaction, p,
  plot = TRUE,
  print.auc = TRUE, show.thres = TRUE
)

auc(roc.ordi)
```


```{r}
p1$predict <- predict(fit_ordinal, type = 'class')


yardstick::roc_curve(p1, patient_satisfaction, `ä¸æ»¡æ„`:`éå¸¸æ»¡æ„`) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = .level)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw()
```



### ä½œä¸ºé¢„æµ‹æ¨¡å‹çš„åˆ†æ

ä¸‹é¢ä¸¤ä¸ªåˆ†æçš„ä¸»è¦ç›®çš„ï¼Œåœ¨äºæ„å»ºé¢„æµ‹æ¨¡å‹ã€‚


```{r}
library(tidymodels)
library(Hmisc)
library(rms)
```


#### ä½œä¸ºä¸´åºŠé¢„æµ‹æ¨¡å‹çš„åˆ†æ

è¯¦ç»†å†…å®¹è§`clinical_prediction_model.Rmd`


#### ä½œä¸ºæœºå™¨å­¦ä¹ é¢„æµ‹æ¨¡å‹

é€»è¾‘å›å½’çš„è¶…å‚æ•°ï¼Œ

mixture = tune(), penalty = tune()







