---
title: "modelStudio"
author: "liuc"
date: '2022-03-01'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## modelStudio

> https://ema.drwhy.ai/

modelStudio是，


`SHAP`(Shapley Additive Explanations)是指，
Shapley values is the only prediction explanation framework with a solid theoretical foundation (Lundberg and Lee (2017)).
在Python中有较好的实现，不过目前还没有在R中见到较好的R包，现在此做一个笔记。

传统的feature importance只告诉哪个特征重要，但我们并不清楚该特征是怎样影响预测结果的。SHAP value最大的优势是SHAP能对于反映出每一个样本中的特征的影响力，而且还表现出影响的正负性。


### SHAP

首先是收集一些R包。

```{r}
library(shapr)
library(SHAPforxgboost)
# library(shapviz)
# library(shapper)
```

```{r}

```


### everybody love modelStudio!!

在诸多模型的构建中，对变量的解释，或者说变量对模型的贡献度需要清晰的解释。
而本包可以对模型本身进行评估。

`break_down`, `Shapley additive explanations`

```{r, include=FALSE}

library(DALEXtra)
library("DALEX")
library("modelStudio")

```

### 做一个测试样本。

```{r}
# load packages and data

data <- DALEX::titanic_imputed

# split the data
index <- sample(1:nrow(data), 0.7*nrow(data))
train <- data[index,]
test <- data[-index,]

# tidymodels fit takes target as factor
train$survived <- as.factor(train$survived)

# fit a model
rec <- recipe(survived ~ ., data = train) %>%
       step_normalize(fare)

clf <- rand_forest(mtry = 2) %>%
       set_engine("ranger") %>%
       set_mode("classification")

wflow <- workflow() %>%
         add_recipe(rec) %>%
         add_model(clf)

model <- wflow %>% fit(data = train)


```


### 对测试模型的使用

Green bars represent positive contributions to the predicted outcome, while red bars represent negative contributions.

```{r}
# create an explainer for the model
# 在测试集数据集测试
explainer <- explain_tidymodels(model,
                                data = test,
                                y = test$survived,
                                label = "tidymodels")

# pick observations
new_observation <- test[1:2,]
rownames(new_observation) <- c("id1", "id2")

# make a studio for the model
modelStudio(explainer, new_observation)
```



### 对`random_forest_tidymodels.Rmd`文件中的final_fit模型进行解释


通过对模型的探索我们知道其没有经过变量筛选的过程，含有9000多个变量。

`explainer`是一个，

```{r}
# create an explainer for the model
# 在测试集数据集测试
explainer <- explain_tidymodels(final_fit,
                                data = df_train,
                                y = df_train$class,
                                label = "tidymodels_RandomForest")

# pick observations
# 先以一个样本数据为例，
new_observation <- df_test %>% 
  head(n = 2) %>% 
  mutate(class = as.numeric(class)) %>% 
  as.data.frame()
rownames(new_observation) <- c("id1", "id2")

# make a studio for the model
modelStudio(explainer, new_observation)
```



Break-down plots can be very helpful in understanding why an individual received a given prediction from a black box model. However, note that the `break-down` is order dependent. If there are interaction effects between two or more predictor variables, you will get different break-down results for different variable orders.
Shapley additive explanations or `SHAP` values get around this reliance on order by calculating break-down contributions for many predictor variable orders and averaging the results for each variable.

```{r}
rf_pparts <- DALEX::predict_parts(explainer = explainer,
                                  new_observation = new_observation,
                                  type = 'break_down' # shap
                                  )
```


```{r}
plot(rf_pparts)
```




