---
title: "modelStudio"
author: "liuc"
date: '2022-03-01'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## modelStudio

model interpretation technique 的记录笔记。

> https://ema.drwhy.ai/
> https://medium.com/dataman-in-ai/explain-your-model-with-lime-5a1a5867b423
> https://medium.com/analytics-vidhya/explain-your-model-with-microsofts-interpretml-5daab1d693b4

modelStudio是，


`SHAP`(Shapley Additive Explanations)是指，
Shapley values is the only prediction explanation framework with a solid theoretical foundation (Lundberg and Lee (2017)).
在Python中有较好的实现，不过目前还没有在R中见到较好的R包，现在此做一个笔记。

传统的feature importance只告诉哪个特征重要，但我们并不清楚该特征是怎样影响预测结果的。SHAP value最大的优势是SHAP能对于反映出每一个样本中的特征的影响力，而且还表现出影响的正负性。


### SHAP

首先是收集一些R包。在R中，主要提供了shapper和fastshap两个包。shapper是python中shap包的一种封装，所以在使用它之前一定要配置好python环境。而fastshap提供了SHAP的近似计算方法。shapforxgboost包提供了专门针对xgboost模型的SHAP值的绘图。

以下将介绍 `fastshap` 包。这是一个更加通用的包，除了xgboost之外，该包还可以用于SVM，randomforest, GBM等模型的解释。

```{r}
library(shapr)
library(SHAPforxgboost)
library(fastshap)
# library(shapviz)
# library(shapper)
library(treeshap)
```

拟合一个模型并解释：
```{r}

```


### Explain Your Model with LIME

Local Interpretable Model-agnostic Explanations

```{r}
library(lime)
library(e1071)
```

```{r}

# Load data
data(iris)
X <- iris[, 1:4]
y <- ifelse(iris$Species == "versicolor", 1, 0)

# Train SVM model
svm_model <- svm(x = X, y = y, kernel = "linear", probability = TRUE)

# Create explainer object
explainer <- lime(X, model = svm_model, bin_continuous = TRUE)

# Select instance to explain
instance_to_explain <- X[1, ]

# Generate explanations
explanations <- explain(instance_to_explain, explainer, n_features = 2, n_labels = 1)

# Print explanations
print(explanations)
```


### Explaining Your Model with Microsoft’s InterpretML

```{r}

```




### everybody love modelStudio!!

在诸多模型的构建中，对变量的解释，或者说变量对模型的贡献度需要清晰的解释。
而本包可以对模型本身进行评估。

`break_down`, `Shapley additive explanations`

```{r, include=FALSE}

library("DALEX")
library(DALEXtra)
library("modelStudio")

```

### 做一个测试样本。

```{r}
# load packages and data

data <- DALEX::titanic_imputed

# split the data
index <- sample(1:nrow(data), 0.7*nrow(data))
train <- data[index,]
test <- data[-index,]

# tidymodels fit takes target as factor
train$survived <- as.factor(train$survived)

# fit a model
rec <- recipe(survived ~ ., data = train) %>%
       step_normalize(fare)

clf <- rand_forest(mtry = 2) %>%
       set_engine("ranger") %>%
       set_mode("classification")

wflow <- workflow() %>%
         add_recipe(rec) %>%
         add_model(clf)

model <- wflow %>% fit(data = train)


```


### 对测试模型的使用

Green bars represent positive contributions to the predicted outcome, while red bars represent negative contributions.

```{r}
# create an explainer for the model
# 在测试集数据集测试
explainer <- explain_tidymodels(model,
                                data = test,
                                y = test$survived,
                                label = "tidymodels")

# pick observations
new_observation <- test[1:2,]
rownames(new_observation) <- c("id1", "id2")

# make a studio for the model
modelStudio(explainer, new_observation)
```



### 对`random_forest_tidymodels.Rmd`文件中的final_fit模型进行解释


通过对模型的探索我们知道其没有经过变量筛选的过程，含有9000多个变量。

`explainer`是一个，

```{r}
# create an explainer for the model
# 在测试集数据集测试
explainer <- explain_tidymodels(final_fit,
                                data = df_train,
                                y = df_train$class,
                                label = "tidymodels_RandomForest")

# pick observations
# 先以一个样本数据为例，
new_observation <- df_test %>% 
  head(n = 2) %>% 
  mutate(class = as.numeric(class)) %>% 
  as.data.frame()
rownames(new_observation) <- c("id1", "id2")

# make a studio for the model
modelStudio(explainer, new_observation)
```



Break-down plots can be very helpful in understanding why an individual received a given prediction from a black box model. However, note that the `break-down` is order dependent. If there are interaction effects between two or more predictor variables, you will get different break-down results for different variable orders.
Shapley additive explanations or `SHAP` values get around this reliance on order by calculating break-down contributions for many predictor variable orders and averaging the results for each variable.

```{r}
rf_pparts <- DALEX::predict_parts(explainer = explainer,
                                  new_observation = new_observation,
                                  type = 'break_down' # shap
                                  )
```


```{r}
plot(rf_pparts)
```


### SVM

```{r}
# Train SVM model
svm_model <- svm(x = X, y = y, kernel = "linear", probability = TRUE)

# Create explainer object
explainer <- explain(svm_model, data = X, y = y, label = "SVM")

# Generate feature importance plot
plot_features_contrib(explainer, type = "contribution", top_n = 4)
```








