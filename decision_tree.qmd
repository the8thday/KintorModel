---
title: "decision tree"
format: html
---

## decision tree

一个不错的数据集搜集的网站。
> https://archive-beta.ics.uci.edu/

> https://scikit-learn.org/stable/modules/tree.html


决策树，classical trees & conditional inference trees.
决策树的一般逻辑过程为，首先是特征选择，选择和分类结果相关性较高的特征，一般通过信息增益等；然后，选择好特征后从根节点出发，对节点计算所有的信息增益,选择信息增益最大的特征作为节点特征，根据该特征的不同取值建立子节点；对每个子节点使用相同的方式生成新的子节点，直到信息增益很小或者没有特征可以选择为止。

根节点如何选择呢？选择信息增益最大或Gini系数最小的分类特征作为根节点以及后续的子节点。还需要确定最优的分类变量和分类阈值等。
在数据集中随机选择一个数据点，并随机分配给它一个数据集中存在的标签，分配错误的概率即为Gini impurity。

*3种典型的决策树算法为*，ID3、C4.5、CART, CART 算法使用了基尼系数取代了信息熵模型。


```{r, include=FALSE}
library(rpart)
library(rpart.plot)
library(rattle)
library(partykit)
```



```{r}
# 9和自变量的小数据集
breast <- read.table('./datasets/breast-cancer-wisconsin.data', sep = ',',
                     header = FALSE,
                     na.strings = '?'
                     )
names(breast) <- c("ID", "clumpThickness", "sizeUniformity", "shapeUniformity", "maginalAdhesion",
                   "singleEpithelialCellSize", "bareNuclei",
                   "blandChromatin", "normalNucleoli","mitosis", "class")
df <- breast[-1]

df$class <- factor(df$class, levels=c(2,4),
                   labels=c("benign", "malignant"))

df <- na.omit(df)
```


```{r}
set.seed(1234)
index <- sample(nrow(df), 0.7*nrow(df))

train_df <- df[index, ]
test_df <- df[-index, ]
```


### 利用`rpart`包进行决策树的构建。

```{r}
dtree <- rpart::rpart(class ~ ., data = train_df,
                      method = 'class',
                      parms=list(split="information")
                      )

# summary(dtree)
```

CP(complexity parameter)
```{r}
dtree$cptable
```
*interpret: *To choose a final tree size, examine the cptable component of the list returned by rpart(). It contains data about the prediction error for various tree sizes. The complexity parameter (cp) is used to penalize larger trees. Tree size is defined by the number of branch splits (nsplit). A tree with n splits has n + 1 terminal nodes. The rel error column contains the error rate for a tree of a given size in the training sample. The cross-validated error (xerror) is based on 10-fold cross-validation (also using the training sample). The xstd column contains the standard error of the cross-validation error.


```{r}
plotcp(dtree)
```
*interpret: *选择多少个split作为



依据上面的分析，得到nsplit个数所对应的CP值，带入进行修剪。
```{r}
dtree.prune <- prune(dtree, cp = 0.01705)
```

```{r}
rattle::fancyRpartPlot(dtree.prune, sub="Classification Tree")
```

```{r}
plot(as.party(dtree.prune), gp = gpar(fontsize = 6))
```


```{r}
dtree.pred <- predict(dtree.pruned, test, type="class")

dtree.perf <- table(test$class, dtree.pred,
                    dnn=c("Actual", "Predicted"))
```


*Conditional inference trees* are similar to traditional trees, but variables and splits are selected based on significance tests rather than purity/homogeneity measures. The significance tests are permutation tests.

Note that pruning isn’t required for conditional inference trees, and the process is somewhat more automated.

```{r}
# conditional inference tree
fit.ctree <- partykit::ctree(class~., data=train_df)
```

```{r}
plot(fit.ctree, main = 'Conditional Inference Tree',
     gp = gpar(fontsize = 6)
     )
```

```{r}
ctree.pred <- predict(fit.ctree, test_df, type="response")

ctree.perf <- table(test_df$class, ctree.pred,
                    dnn=c("Actual", "Predicted"))

ctree.perf
```




### tidymodels中的决策树构建

具体参见`random_forest_tidymodels.Rmd`文件。













