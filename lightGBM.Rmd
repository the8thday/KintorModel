---
title: "lightGBM"
author: "liuc"
date: "1/17/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## lightGBM

https://www.kaggle.com/code/athosdamiani/lightgbm-with-tidymodels/notebook

以一个分类问题进行示例。

`randomforest` & `xgboost`都可以确定特征重要性，so？SHAPLEY?



```{r, include=FALSE}
library(doParallel)
library(tidyverse)
library(tidymodels)
library(bonsai)
# library(treesnip) # 被bonsai代替了


show_engines('boost_tree')

all_cores <- parallel::detectCores(logical = FALSE)
registerDoParallel(cores = all_cores)
```


### 理解数据

```{r}
adult <- read_csv("./datasets/adult.csv")
adult <- adult %>% 
    janitor::clean_names() 

glimpse(adult)
```

```{r}
paint::paint(adult)
```


### Step 1: train/test split ----------------------------------------

测试数据，暂不考虑数据整理的部分。谈论起数据整理，Pandas/Numpy生态提供的数据格式和工具都是属于数据整理应用的范畴。

```{r}
set.seed(42)

adult_initial_split <- initial_split(adult, strata = "income", prop = 0.75)

adult_train <- training(adult_initial_split)
adult_test  <- testing(adult_initial_split)

adult_initial_split

```


### Step 3: dataprep --------------------------------------------------------

数据输入前的处理。此处因为摘抄的流程，故不涉及详细的整理过程。

`step_dummy`是有必要的吗？

```{r}
adult_recipe <- recipe(income ~ ., data = adult_train) %>%
  step_impute_mode(workclass, occupation, native_country) %>%
  step_zv(all_predictors()) %>%
  step_novel(all_nominal(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes())

head(juice(prep(adult_recipe)))
```


### Step 4: model definiton -----------------------------------

超参数还是最为需要关注的地方，每个参数设置的范围和是否需要设置等。

```{r}
adult_model <- 
  boost_tree(
  mtry = 5, 
  trees = 1000, 
  min_n = tune(), 
  tree_depth = tune(),
  loss_reduction = tune(), 
  learn_rate = tune(), 
  sample_size = 0.75
) %>% 
  set_mode("classification") %>%
  set_engine("lightgbm")

# workflow
adult_wf <- workflow() %>% 
    add_model(adult_model) %>% 
    add_recipe(adult_recipe)

adult_wf
```


### Step 5: hiperparameter tunning ------------------------------------------

```{r}
# resamples
adult_resamples <- vfold_cv(adult_train, v = 5)

# grid
# 构建一个grid参数表
adult_grid <- hardhat::extract_parameter_set_dials(adult_model) %>% 
    finalize(adult_train) %>% 
    grid_random(size = 200)
head(adult_grid)
```
```{r}
# grid search
# 相比xgboost，lightgbm快不少，大概十几分钟的样子
adult_tune_grid <- adult_wf %>%
    tune_grid(
        resamples = adult_resamples,
        grid = adult_grid,
        control = control_grid(verbose = FALSE),
        metrics = metric_set(roc_auc)
    )

autoplot(adult_tune_grid)
```
*interpret: *图中的结果为四个tune的参数。learning rate似乎越大模型表现越好，minimal node size/loss reduction取值影响不大；tree depth在达到8之后影响便几乎不在变化。


```{r}
# top 5 hiperparams set
show_best(adult_tune_grid, "roc_auc")
```
把通过grid search得到的参数带入到模型中便是选择好超参数需要用训练集数据训练的模型。



### Step 6: last fit performance ------------------------------------------

开始模型的训练。

```{r}
# select best hiperparameter found
adult_best_params <- select_best(adult_tune_grid, "roc_auc")
adult_wf <- adult_wf %>% finalize_workflow(adult_best_params)

# last fit
# 在训练集上训练，并在测试集上evaluate
adult_last_fit <- last_fit(
  adult_wf,
  adult_initial_split
)

# metrics
collect_metrics(adult_last_fit)
```

一个插曲，不使用`last_fit`函数，而是用模型直接拟合训练集数据：
```{r, eval=FALSE}
# 不需要运行

```


```{r}
# roc curve
adult_test_preds <- collect_predictions(adult_last_fit)
adult_roc_curve <- adult_test_preds %>% roc_curve(income, `.pred_<=50K`)
autoplot(adult_roc_curve)

# confusion matrix
adult_test_preds %>%
  mutate(
    income_class = factor(if_else(`.pred_<=50K` > 0.6, "<=50K", ">50K"))
  ) %>%
  conf_mat(income, income_class)
```
```{r}
yardstick::roc_auc()
```


### SHAP

```{r}
library(shapr)
```


### 模型保存

得到的模型在经过验证后才是所需要的最后的结果，需要保存并部署。

```{r}
adult_wf_model <- adult_last_fit$.workflow[[1]]
predict(adult_wf_model, adult_test[10, ])


saveRDS(adult_wf_model, here::here("final_model.rds"))

```


读入保存的模型，并做预测：

```{r}

```










